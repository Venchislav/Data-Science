{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSnCSG2vuwHq8GsGkc7Ged",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venchislav/Data-Science/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imagine  we  have dataset with 2 columns spendings and sales\n",
        "# spendings is a feature columns and sales is a target\n",
        "# We decided to use linear_regression\n",
        "# And we also decided to use Gradient Descent optimization"
      ],
      "metadata": {
        "id": "3WrARN98A-Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know that linear regression model is: <br>\n",
        "# 𝒇(𝑥) = 𝑤𝑥 + 𝒃"
      ],
      "metadata": {
        "id": "xPBtgG71BYtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the main target for us now is to find 𝑤 and 𝒃"
      ],
      "metadata": {
        "id": "O22wvDqwCjMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhILjGkUA1n-"
      },
      "outputs": [],
      "source": [
        "# we are gonna do it by minimizing (MSE):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1/N Σ(yᵢ − (𝑤𝑥 + 𝒃))^2\n",
        "\n",
        "Where:\n",
        "N - total amount of points\n",
        "yᵢ - real value\n",
        "(𝑤𝑥 + 𝒃) - value predicted by linear regression"
      ],
      "metadata": {
        "id": "QDEtqYmDET4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1/N Σ(yi − (𝑤𝑥 + 𝒃))^2"
      ],
      "metadata": {
        "id": "Ay0mmApYD26M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# of course we can write our own implementation of Gradient Descent,\n",
        "# but we mostly use libraries, such as sklearn\n",
        "\n",
        "# There we can use SGDRegressor with l1 param(meanining linear regression)"
      ],
      "metadata": {
        "id": "GL8wzO1LeeOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor"
      ],
      "metadata": {
        "id": "Ty6Ut_3SfKjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "2og6dapJhSTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([37.8, 39.3, 45.9, 41.3])\n",
        "y = np.array([22.1, 10.4, 9.3, 18.5])"
      ],
      "metadata": {
        "id": "38jHdKOUfW8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regr = SGDRegressor(loss='squared_error', penalty=None)"
      ],
      "metadata": {
        "id": "Cd9XTNlhf58L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}